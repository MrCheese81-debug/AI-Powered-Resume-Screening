# -*- coding: utf-8 -*-
"""AI Resume Screening

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RUgK2ubvDKlowvaaZqst0X1mAkyPWo6V
"""

import pandas as pd
import numpy as np

data = pd.read_csv('/content/AI_Resume_Screening.csv')

"""DATA EXPLORATION"""

data.copy().head()

data.shape

data.info()

data['Job Role'].value_counts()

"""DATA PREPROCESSING"""

data.drop(['Resume_ID', 'Name', 'Recruiter Decision', 'Salary Expectation ($)', 'AI Score (0-100)'], axis=1, inplace=True)

data.info()

data.isnull().sum()

data['Certifications'] = data['Certifications'].fillna(value='None')
data['Certifications'].isnull().sum()

data.isnull().sum()

data['Certifications'].unique()

duplicate_rows = data[data.duplicated()]
print("Duplicated Rows:")
print(duplicate_rows)

data_no_duplicates = data.drop_duplicates()
print("\nDataframe after removing duplicates:")

data['Skills'].unique()

from sklearn.preprocessing import MultiLabelBinarizer
data['Skills_List'] = data['Skills'].apply(lambda x: [skill.strip() for skill in x.split(',')])

mlb = MultiLabelBinarizer()
skills_binary = mlb.fit_transform(data['Skills_List'])

skills_labeled = pd.DataFrame(skills_binary,
                              columns=mlb.classes_,
                              index=data.index)

data = pd.concat([data, skills_labeled], axis=1)

data.drop(['Skills', 'Skills_List'], axis=1, inplace=True)

data.head()

data = pd.get_dummies(data, columns=['Certifications'], dtype=int)

data.info()

data = pd.get_dummies(data, columns=['Education'], dtype=int)

data.info()

data.head()

from sklearn.feature_selection import SelectKBest, f_classif

features = ["Experience (Years)", "Projects Count",  "C++", "Cybersecurity", "Deep Learning", "Ethical Hacking", "Java", "Linux",
            "Machine Learning", "NLP", "Networking", "Python", "Pytorch", "React", "SQL", "TensorFlow",
            "Certifications_AWS Certified", "Certifications_Deep Learning Specialization", "Certifications_Google ML",
            "Education_B.Sc", "Education_B.Tech", "Education_M.Tech", "Education_MBA", "Education_PhD"]

X = data[features]
y = data["Job Role"]

selector = SelectKBest(score_func=f_classif, k=5)
X_new = selector.fit_transform(X, y)

selected_features = np.array(features)[selector.get_support()]
print("Selected features:", selected_features)

features = ["Experience (Years)", "Projects Count", "Ethical Hacking", "Java", "Machine Learning", "NLP", "TensorFlow",
            "Education_B.Sc", "Education_B.Tech", "Education_M.Tech", "Education_MBA", "Education_PhD"]
X = data[features]
y = data["Job Role"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

preprocessor = ColumnTransformer([
   ("num", StandardScaler(), ["Experience (Years)", "Projects Count"]),
   ("cat", "passthrough", [ "Ethical Hacking", "Java", "Machine Learning", "NLP", "TensorFlow",
            "Education_B.Sc", "Education_B.Tech", "Education_M.Tech", "Education_MBA", "Education_PhD"])
])

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score

print("\nTraining Random Forest")
rf_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=5, random_state=42))
])

rf_pipeline.fit(X_train, y_train)
y_pred = rf_pipeline.predict(X_test)

print("Random Forest Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("F1-score:", f1_score(y_test, y_pred, average='weighted'))
print("")
print(classification_report(y_test, y_pred))

rf_accuracy = accuracy_score(y_test, y_pred)
print(f"Random Forest Accuracy: {rf_accuracy}")

print("\nTraining K-Nearest Neighbors")


knn_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", KNeighborsClassifier(n_neighbors=5))
])

knn_pipeline.fit(X_train, y_train)
y_pred = knn_pipeline.predict(X_test)

print("KNN Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("F1-score:", f1_score(y_test, y_pred, average='weighted'))
print("")
print(classification_report(y_test, y_pred))

KNN_accuracy = accuracy_score(y_test, y_pred)
print(f"K-Nearest Neightbors Accuracy: {KNN_accuracy}")

print("\nTraining Artificial Neural Network")

ann_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam'
    , max_iter=500, random_state=42))
])

ann_pipeline.fit(X_train, y_train)
y_pred = ann_pipeline.predict(X_test)

print("ANN Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("F1-score:", f1_score(y_test, y_pred, average='weighted'))
print("")
print(classification_report(y_test, y_pred))

ANN_accuracy = accuracy_score(y_test, y_pred)
print(f"Artificial Neural Network Accuracy: {ANN_accuracy}")

print("\nTraining Decision Tree")


DT_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", DecisionTreeClassifier(criterion='gini',max_depth=5, min_samples_split=2, random_state=42))
])


DT_pipeline.fit(X_train, y_train)
y_pred = DT_pipeline.predict(X_test)

print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("F1-score:", f1_score(y_test, y_pred, average='weighted'))
print("")
print(classification_report(y_test, y_pred))

DT_accuracy = accuracy_score(y_test, y_pred)
print(f"Decision Tree Accuracy: {DT_accuracy}")

models = {
    "Random Forest": rf_pipeline, "K-Nearest Neighbors": knn_pipeline, "Artificial Neural Network": ann_pipeline, "Decision Tree": DT_pipeline,
}

from sklearn.model_selection import KFold, cross_val_score
kf = KFold(n_splits=5, shuffle=True, random_state=42)

cv_results = {}

for name, pipeline in models.items():
    scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='accuracy')

    mean_accuracy = scores.mean()

    cv_results[name] = mean_accuracy
    print(f"{name}: Mean Accuracy = {mean_accuracy:.4f}")

cv_accuracy_scores = cv_results

test_accuracies = {
    "Random Forest": rf_accuracy,
    "K-Nearest Neighbors": KNN_accuracy,
    "Artificial Neural Network": ANN_accuracy,
    "Decision Tree": DT_accuracy,

}

cv_accuracies = cv_accuracy_scores

accuracy_df = pd.DataFrame({
    "Model": test_accuracies.keys(),
    "Test Accuracy": test_accuracies.values(),
    "Cross-Validation Accuracy": cv_accuracies.values()
})

accuracy_df = accuracy_df.melt(id_vars=["Model"], var_name="Metric", value_name="Accuracy")

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))

sns.barplot(x="Model", y="Accuracy", hue="Metric", data=accuracy_df, palette="coolwarm")

plt.ylim(0, 1)
plt.title("Test Accuracy vs. Cross-Validation Accuracy")
plt.xlabel("Model")
plt.ylabel("Accuracy Score")
plt.legend(title="Metric")
plt.show()

from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

y_test_binary = (y_test == 'Data Scientist').astype(int)

plt.figure(figsize=(8, 6))

for name, model in models.items():
    class_index = list(model.classes_).index('Data Scientist')
    y_pred_proba = model.predict_proba(X_test)[:, class_index]

    precision, recall, _ = precision_recall_curve(y_test_binary, y_pred_proba)
    plt.plot(recall, precision, label=name)

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve for 'Data Scientist'")
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

y_test_binary = (y_test == 'Data Scientist').astype(int)

plt.figure(figsize=(8, 6))

for name, model in models.items():
    class_index = list(model.classes_).index('Data Scientist')

    y_pred_proba = model.predict_proba(X_test)[:, class_index]

    fpr, tpr, _ = roc_curve(y_test_binary, y_pred_proba)
    roc_auc = auc(fpr, tpr)

    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], linestyle="--", color="gray")

plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for 'Data Scientist'")
plt.legend()
plt.grid(True)
plt.show()

model_names = list(test_accuracies.keys())
accuracies = list(test_accuracies.values())

plt.figure(figsize=(8, 5))
sns.barplot(x=accuracies, y=model_names, palette="coolwarm")

plt.xlabel("Accuracy Score")
plt.ylabel("Model")
plt.title("Model Accuracy Comparison")
plt.xlim(0, 1)
plt.show()